---
title: "R Notebook"
output:
  word_document: default
  html_document:
    df_print: paged
---


## Introduction

For this report, the relationship between the cost of computer memory over years in time is investigated. A dataset of memory price per megabyte at different times of the year from 1957 to 2018 is provided through the url: https://jcmit.net/memoryprice.htm. Through an analysis of the dataset, the goal of this investigation is to examine patterns of fluctuation in the cost of computer memory, and with the result, reasonably predict future costs.

Before the analysis begins, it is identified that there are two possible methods to read the dataset. The first way is to extract usable data through the given Excel file, as provided in the link. This method requires some tidying to separate the same file into multiple tables. The second way is to directly scrape the table off the HTML site, referencing the table element during the process. An original attempt to read straight from the web, but a couple of rows were missing and the whole process was complicated. Thus, the approach to read directly from the .xls file was chosen for the purpose of this report.

The computer memory data required for the purpose of this analysis is in the MEMORY sheet. There are 14 columns and 359 rows. The headers take up 4 rows, so there should really be 355 rows of data with one row for the header. The following is a brief summary of the columns:

  1. The fractional (decimal) year the entry was recorded
  2. Cost per megabyte
  3. The year the entry was recorded
  4. The month the entry was recorded
  5. The type of the catalogue where the memory unit was sold
  6. The page where the advert can be found in the catalogue
  7. The store where the memory unit was sold
  8. The size of the memory unit in kilobytes
  9. The cost (in USD) of the memory unit
  10. Timings (speed) on the memory unit
  11. Type of memory the unit uses
  12. The brand of the memory unit. There appears to be comments and a bunch of other data types in this column
  13. What seems to be an outdated price column. Some of the prices are also defined along the memory type, which all match with column 9 but not this column.
  14. What seems to be an outdated timings (speed) column. Again, there seems to be more accurate data in column 10, and the data in column 10 seems to be more complete.

To ensure the flow of this report, the report will consist of a combination of R code and explanations. Then begin the extraction of the data, where the first step is to load in dependencies. The required packages for the setup are gdata and tidyverse.
```{r warning=FALSE, message=FALSE}

require("gdata")
require("tidyverse")

```

When the setup is ready, read the data. There are a few issues to note with the data frame that's been read in:

1. The data frame has 16 columns, whilst the needed data only fills 14 of the columns. It looks like two empty columns were read in at the end. The resolution is just to remove them.

2. There were 4 rows for the header in the data, but the dataframe took the first row as the header and the rest as data. A step is needed to remove the first three rows in the data.

3. The headers poorly represent the data, and new headers will need to be defined.

```{r warning=FALSE, message=FALSE}

#Read data
memory = read.xls("MemDiskPrice-xl95.xls", sheet = 1, header = TRUE)

#Display data
head(memory, n = 10)

```

## Methods
After reading the data into a dataframe, some tidying cna be done to the data. Note that some row indices in the resulting dataframe are out of order, given some rows are removed, and needs an update:
```{r warning=FALSE, message=FALSE}

#Remove the last 2 columns
memory = subset(memory, select=-c(`X.11`, `X.12`))

#Remove the first 3 rows
memory = tail(memory, -3)

#Re-define column names
colnames(memory) = c("frac_year", "cost_per_Mb", "year", "month", "cat_type", "page", "store", "KB", "unit_cost", "clock", "mem_type", "brand", "price_outdated", "clock_outdated")

#Remove row names (indexes)
rownames(memory) = NULL

#Display data
head(memory, n = 10)

```

There are still some unneeded columns in the dataframe. It is observed that the last two outdated columns can be removed since they contain outdated data. The page column contains the page number in a particular magazine, where information on the storage device can be found. Along with the brand column, they won't be much use to the analysis of memory prices through time, and can be disregarded for the purpose of this analysis. At last, numerical columns are converted from factor to numericals.
```{r warning=FALSE, message=FALSE}

#Remove the outdated columns
memory = memory[-14]
memory = memory[-13]

#Remove catalogue, store, and page data
memory = subset(memory, select=-c(cat_type, page, store, brand))

#Remove entries with null months
memory = memory[!(is.na(memory$month) | memory$month==""), ]

#Convert to numeric
memory$frac_year = as.numeric(gsub(",", "", memory$frac_year))
memory$cost_per_Mb = as.numeric(gsub(",", "", memory$cost_per_Mb))
memory$KB = as.numeric(gsub(",", "", memory$KB))
memory$unit_cost = as.numeric(gsub(",", "", memory$unit_cost))

head(memory, n = 10)

```


## Analysis
The first part of the analysis comprises of an attempt that was made to assign linear model to the data, which had an abysmally small R-squared value of 0.04. This means that the linear model captures only approximately 5% of the data.

```{r warning=FALSE, message=FALSE}
memory_lm = lm(cost_per_Mb~frac_year,data=memory)
summary(memory_lm)
```

Then, a plot was made with the cost of memory per Mb against year. Observe that the resulting plot followed a decreasing convex curve. However, notice that the p-value was extremely small, which means that it is probable that a linear relationship exists. Since the plot followed a decreasing convex pattern, taking the logarithm of the price per Mb variable could normalize the data. Taking the logarithm on cost per Mb and replotting the data, a very obvious linear trend appears. Running the linear model on the normalized data, an R-squared adjusted value of 0.9743 and a more significant p-value are given. This suggests that the linear model fits well for the dataset.

```{r warning=FALSE, message=FALSE}

#Plot fractional year against cost per Mb
ggplot(data=memory, aes(x=frac_year, y=cost_per_Mb)) + 
  geom_point() +
  scale_x_continuous(breaks = round(seq(min(memory$frac_year), max(memory$frac_year), by = 5),1)) +
  scale_y_log10(labels = function(x) format(x, big.mark = ",", scientific = FALSE))

#Fit a linear model
memory_log_lm= lm(log(cost_per_Mb)~frac_year,data=memory)
summary(memory_log_lm)

```

Below shows the differences in cost per Mb over the years. It is easy to see that, even though there are certain years with zero cost difference, as time progresses, the cost differences follow a negative linear relationship.
```{r warning=FALSE, message=FALSE}
#Put the month data into proper format
memory$month = substring(memory$month, 1, 3)

#Calculate differences in cost per Mb every year
cost_diff = memory[-nrow(memory),]$cost_per_Mb
cost_diff_subtract = memory[-1,]$cost_per_Mb

#Add NA as first entry as we do not know what the cost difference is between the first year in our data and the year before
cost_diff = cost_diff_subtract - cost_diff
cost_diff = append(cost_diff, NA, after = 0)

#Add difference column to data
memory$cost_diff = cost_diff

#Graph the cost differences against time
ggplot(data=memory, aes(x=frac_year, y=cost_diff)) + 
  geom_point() +
  scale_x_continuous(breaks = round(seq(min(memory$frac_year), max(memory$frac_year), by = 5),1)) +
  scale_y_log10(labels = function(x) format(x, big.mark = ",", scientific = FALSE))

```

In order to perform a t-test, a test must first be applied to verify the normality assumption.
```{r warning=FALSE, message=FALSE}
ggplot(memory, aes(sample = log(cost_per_Mb))) + stat_qq() + stat_qq_line()
```
As the Normal Q-Q plot show, the points do not fall perfectly on the normal line. The points at both ends of the data set fall far from the normal line, indicating a 'heavy tail' distribution. There is no indication that the data is normally distributed. A t-test would not be appropriate.

By plotting the residual values against the fitted values of the model, it is shown that the data points are not distributed randomly. This also means the errors of the log linear model are heteroscedastic, and fails one of the assumptions for a Tukey HSD test.
```{r warning=FALSE, message=FALSE}

mem_res = resid(memory_log_lm)
mem_fit_val = fitted(memory_log_lm)
mem_res_plot = data.frame(cbind(mem_res, mem_fit_val))
ggplot(mem_res_plot, aes(x = mem_fit_val, y = mem_res)) + geom_point() + geom_hline(yintercept = 0)

```

When categorized by the month when the memory cost is recorded, peaks of median log costs can be seen at certain months, particularly in January, March, and August. 
```{r warning=FALSE, message=FALSE}
ggplot(memory, aes(month, log(cost_per_Mb))) + geom_boxplot() + scale_x_discrete(limits = month.abb)
```

Since normality is not violated, and the spreads of each month group are different, it makes sense to perform a Mood's median test (using the smmr package by Ken Butler). The p-value of the result is extremely close to the significant value 0.05. None of the other pairwise median comparisons show significant results. There is no strong evidence suggesting that January and February have median differences between any other month. That being said, it is valid to treat the p-value as insignificant. There does not exist any significant differences in mean costs per Mb between different months.

```{r warning=FALSE, message=FALSE}
library(smmr)
median_test(memory, log(cost_per_Mb), month)

mem_pairwise_res = pairwise_median_test(memory, log(cost_per_Mb), month)
mem_pairwise_res[mem_pairwise_res$p_value < 0.05,]
```

## Conclusion
Through the investigation, it is discovered that a linear model captures only approximately 5% of the data and is not appropriate to represent the model. A more appropriate option would be a log linear model.

The key summaries are as follows:

-as time progresses, the cost differences for memory storage follow a negative linear relationship;
-there is no indication that the data is normally distributed;
-the errors of the log linear model are heteroscedastic;
-at certain months, particularly in January, March, and August, median log costs are higher;
-Mood’s median test - none of the other pairwise median comparisons show significant results. There does not exist any significant differences in mean costs per Mb of memory between different months.

**Predictions**

  Given the above observations, the cost per Mb of memory decreases as years pass. It is reasonable to extend the same model for future predictions. If the current trend continues through the log linear model, the cost of memory will decrease as every additional year passes. Eventually, according to the log linear model, the cost of memory will fall below the zero line. This isn't plausible, as a cost value under 0 is meaningless in the context of this investigation. A more plausible model could be a non-linear model that converges to zero as year increases.
